
import csv 
import numpy as np 
import matplotlib.pyplot as plt 
  
# step 1 : Load CSV file
def loadCSV(filename): 
    ''' 
    function to load dataset 
    '''
    with open(filename,"r") as csvfile: 
        lines = csv.reader(csvfile) 
        dataset = list(lines) 
        for i in range(len(dataset)): 
            dataset[i] = [float(x) for x in dataset[i]]      
    return np.array(dataset) 
  
  # step 2 : Normalize 
def normalize(X): 
    ''' 
    function to normalize feature matrix, X 
    '''
    mins = np.min(X, axis = 0) 
    maxs = np.max(X, axis = 0) 
    rng = maxs - mins 
    norm_X = 1 - ((maxs - X)/rng) 
    return norm_X 
  
  # step 3 :  
def logistic_func(beta, X): 
    ''' 
    logistic(sigmoid) function 
    '''
    # below is the code for 1/1+e^(-(bo*x1+ b1*x2 +  b1*x3............ ))
    return 1.0/(1 + np.exp(-np.dot(X, beta.T))) 
  
  
def log_gradient(beta, X, y): 
    ''' 
    logistic gradient function 
    '''
    #first_calc = y_prediction - y_actual  for all samples 
    first_calc = logistic_func(beta, X) - y.reshape(X.shape[0], -1) 
    # now in below step we will find the partial derivative
    #final_calc= gradient is (y_prediction - y_actual)*x  for all samples 
    final_calc = np.dot(first_calc.T, X) 
    
    return final_calc 
  
  
def cost_func(beta, X, y): 
    ''' 
    cost function, J 
    '''
    #y_prediction=  1/1+e^(-(bo*x1+ b1*x2 +  b1*x3............ )) for all samples
    y_prediction= logistic_func(beta, X) 
    y = np.squeeze(y) 
    # calculate cross entropy cost function for all samples
    cost_function = -(y * np.log(y_prediction)) - ((1 - y) * np.log(1 - y_prediction) ) 
    # return the sum of  cost function divided by no. of samples
    return np.mean(cost_function) 
  
  
def train(X, y, beta, lr=.01, converge_change=.001): 
    ''' 
    gradient descent function 
    '''
    cost = cost_func(beta, X, y) 
    change_cost = 1
    num_iter = 1
      
    while(change_cost > converge_change): 
        old_cost = cost 
        #beta= beta - learning_rate * partial derivative of cost function w.r.t beta
        beta = beta - (lr * log_gradient(beta, X, y)) 
        # again calculate cost function
        cost = cost_func(beta, X, y) 
        # find difference between old cost and new cost 
        #if change is greater than .001 then reiterate 
        change_cost = old_cost - cost 
        num_iter += 1
      
    return beta, num_iter  
  
  
def pred_values(beta, X): 
    ''' 
    function to predict labels 
    '''
    pred_prob = logistic_func(beta, X) 
    pred_value = np.where(pred_prob >= .5, 1, 0) 
    return np.squeeze(pred_value) 
  
  
def plot_reg(X, y, beta): 
    ''' 
    function to plot decision boundary 
    '''
    # labelled observations 
    x_0 = X[np.where(y == 0.0)] 
    x_1 = X[np.where(y == 1.0)] 
      
    # plotting points with diff color for diff label 
    plt.scatter([x_0[:, 1]], [x_0[:, 2]], c='b', label='y = 0') 
    plt.scatter([x_1[:, 1]], [x_1[:, 2]], c='r', label='y = 1') 
      
    # plotting decision boundary 
    x1 = np.arange(0, 1, 0.1) 
    x2 = -(beta[0,0] + beta[0,1]*x1)/beta[0,2] 
    plt.plot(x1, x2, c='k', label='reg line') 
  
    plt.xlabel('x1') 
    plt.ylabel('x2') 
    plt.legend() 
    plt.show() 
      
  
      
if __name__ == "__main__": 
    # load the dataset 
    dataset = loadCSV('/content/dataset1.csv') 
      
    # normalizing feature matrix 
    X = normalize(dataset[:, :-1]) 
    print(X)
      
    # stacking columns wth all ones in feature matrix 
    X = np.hstack((np.matrix(np.ones(X.shape[0])).T, X))
    print(X)
  
    # response vector 
    y = dataset[:, -1] 
  
    # initial beta values 
    beta = np.matrix(np.zeros(X.shape[1])) 
  
    # beta values after running gradient descent 
    beta, num_iter =train(X, y, beta) 
  
    # estimated beta values and number of iterations 
    print("Estimated regression coefficients:", beta) 
    print("No. of iterations:", num_iter) 
  
    # predicted labels 
    y_pred = pred_values(beta, X) 
      
    # number of correctly predicted labels 
    print("Correctly predicted labels:", np.sum(y == y_pred)) 
      
    # plotting regression line 
    plot_reg(X, y, beta) 
     
[[0.45109895 0.1999973 ]
 [0.19242517 0.05065823]
 [0.41640382 0.09866732]
 [0.6119831  0.17599276]
 [0.69084812 0.29600195]
 [0.6119831  0.27999892]
 [0.68453991 0.35733787]
 [0.74448032 0.44266483]
 [0.73501801 0.33867218]
 [0.90536447 0.4826724 ]
 [0.81071647 0.36800205]
 [0.83911585 0.4640067 ]
 [0.85488637 0.61599492]
 [0.70661863 0.51200227]
 [0.55520926 0.3813289 ]
 [0.44479074 0.25066905]
 [0.25867378 0.11999567]
 [0.19242517 0.01332685]
 [0.         0.        ]
 [0.15142184 0.07732544]
 [0.29653547 0.02932987]
 [0.40694152 0.05333441]
 [0.49211474 0.09600465]
 [0.30284368 0.21866299]
 [0.52996397 0.32266915]
 [0.6687694  0.3920066 ]
 [0.6687694  0.43466331]
 [0.78548365 0.05065823]
 [0.84226996 0.16800476]
 [0.65299889 0.08266429]
 [0.44794485 0.03199254]
 [0.49526885 0.19467196]
 [0.57728797 0.25867056]
 [0.63721591 0.37332739]
 [0.77917545 0.48533506]
 [0.84857816 0.55999784]
 [0.92428908 0.61333225]
 [0.9495219  0.53333063]
 [0.97790882 0.5733382 ]
 [0.86750277 0.35733787]
 [0.76025083 0.27999892]
 [0.57413387 0.23466602]
 [0.50157705 0.20533614]
 [0.71608093 0.36000054]
 [0.84542406 0.45600519]
 [0.88958149 0.62399643]
 [1.         0.7280026 ]
 [0.86750277 0.55199632]
 [0.70977273 0.54933366]
 [0.64037001 0.48533506]
 [0.22713276 0.46133052]
 [0.12934312 0.335996  ]
 [0.09778964 0.2453302 ]
 [0.42586613 0.44000216]
 [0.31546009 0.34933636]
 [0.57097977 0.55467251]
 [0.40063331 0.49599924]
 [0.2870607  0.44000216]
 [0.40063331 0.57066202]
 [0.52365577 0.664004  ]
 [0.49842295 0.59466656]
 [0.63090771 0.71466223]
 [0.60567489 0.65333982]
 [0.72238914 0.75733247]
 [0.60252079 0.74132944]
 [0.71292683 0.80800422]
 [0.78548365 0.85866245]
 [0.78548365 0.76533398]
 [0.9589842  0.87200281]
 [0.85488637 0.81332955]
 [0.85488637 0.89333117]
 [0.52365577 0.86934015]
 [0.41324972 0.72000108]
 [0.32176829 0.62667261]
 [0.18296286 0.52000378]
 [0.11040604 0.40799611]
 [0.         0.30666613]
 [0.         0.62399643]
 [0.13249723 0.69867272]
 [0.06309451 0.51200227]
 [0.18927107 0.76799665]
 [0.24605737 0.664004  ]
 [0.3880169  0.81066688]
 [0.47003603 0.8186684 ]
 [0.53311807 0.94400292]
 [0.6624612  0.85333712]
 [0.56782566 0.79466386]
 [0.63406181 0.99199849]
 [0.39747921 0.86133863]
 [0.18611696 0.81600573]
 [0.0536322  0.72533993]
 [0.10409784 0.57866353]
 [0.19242517 0.63199795]
 [0.2839066  0.50400076]
 [0.47003603 0.62667261]
 [0.63406181 0.83733409]
 [0.47003603 0.68799503]
 [0.83280765 0.97333279]
 [0.46686946 1.        ]
 [0.46686946 0.94593572]]
[[1.         0.45109895 0.1999973 ]
 [1.         0.19242517 0.05065823]
 [1.         0.41640382 0.09866732]
 [1.         0.6119831  0.17599276]
 [1.         0.69084812 0.29600195]
 [1.         0.6119831  0.27999892]
 [1.         0.68453991 0.35733787]
 [1.         0.74448032 0.44266483]
 [1.         0.73501801 0.33867218]
 [1.         0.90536447 0.4826724 ]
 [1.         0.81071647 0.36800205]
 [1.         0.83911585 0.4640067 ]
 [1.         0.85488637 0.61599492]
 [1.         0.70661863 0.51200227]
 [1.         0.55520926 0.3813289 ]
 [1.         0.44479074 0.25066905]
 [1.         0.25867378 0.11999567]
 [1.         0.19242517 0.01332685]
 [1.         0.         0.        ]
 [1.         0.15142184 0.07732544]
 [1.         0.29653547 0.02932987]
 [1.         0.40694152 0.05333441]
 [1.         0.49211474 0.09600465]
 [1.         0.30284368 0.21866299]
 [1.         0.52996397 0.32266915]
 [1.         0.6687694  0.3920066 ]
 [1.         0.6687694  0.43466331]
 [1.         0.78548365 0.05065823]
 [1.         0.84226996 0.16800476]
 [1.         0.65299889 0.08266429]
 [1.         0.44794485 0.03199254]
 [1.         0.49526885 0.19467196]
 [1.         0.57728797 0.25867056]
 [1.         0.63721591 0.37332739]
 [1.         0.77917545 0.48533506]
 [1.         0.84857816 0.55999784]
 [1.         0.92428908 0.61333225]
 [1.         0.9495219  0.53333063]
 [1.         0.97790882 0.5733382 ]
 [1.         0.86750277 0.35733787]
 [1.         0.76025083 0.27999892]
 [1.         0.57413387 0.23466602]
 [1.         0.50157705 0.20533614]
 [1.         0.71608093 0.36000054]
 [1.         0.84542406 0.45600519]
 [1.         0.88958149 0.62399643]
 [1.         1.         0.7280026 ]
 [1.         0.86750277 0.55199632]
 [1.         0.70977273 0.54933366]
 [1.         0.64037001 0.48533506]
 [1.         0.22713276 0.46133052]
 [1.         0.12934312 0.335996  ]
 [1.         0.09778964 0.2453302 ]
 [1.         0.42586613 0.44000216]
 [1.         0.31546009 0.34933636]
 [1.         0.57097977 0.55467251]
 [1.         0.40063331 0.49599924]
 [1.         0.2870607  0.44000216]
 [1.         0.40063331 0.57066202]
 [1.         0.52365577 0.664004  ]
 [1.         0.49842295 0.59466656]
 [1.         0.63090771 0.71466223]
 [1.         0.60567489 0.65333982]
 [1.         0.72238914 0.75733247]
 [1.         0.60252079 0.74132944]
 [1.         0.71292683 0.80800422]
 [1.         0.78548365 0.85866245]
 [1.         0.78548365 0.76533398]
 [1.         0.9589842  0.87200281]
 [1.         0.85488637 0.81332955]
 [1.         0.85488637 0.89333117]
 [1.         0.52365577 0.86934015]
 [1.         0.41324972 0.72000108]
 [1.         0.32176829 0.62667261]
 [1.         0.18296286 0.52000378]
 [1.         0.11040604 0.40799611]
 [1.         0.         0.30666613]
 [1.         0.         0.62399643]
 [1.         0.13249723 0.69867272]
 [1.         0.06309451 0.51200227]
 [1.         0.18927107 0.76799665]
 [1.         0.24605737 0.664004  ]
 [1.         0.3880169  0.81066688]
 [1.         0.47003603 0.8186684 ]
 [1.         0.53311807 0.94400292]
 [1.         0.6624612  0.85333712]
 [1.         0.56782566 0.79466386]
 [1.         0.63406181 0.99199849]
 [1.         0.39747921 0.86133863]
 [1.         0.18611696 0.81600573]
 [1.         0.0536322  0.72533993]
 [1.         0.10409784 0.57866353]
 [1.         0.19242517 0.63199795]
 [1.         0.2839066  0.50400076]
 [1.         0.47003603 0.62667261]
 [1.         0.63406181 0.83733409]
 [1.         0.47003603 0.68799503]
 [1.         0.83280765 0.97333279]
 [1.         0.46686946 1.        ]
 [1.         0.46686946 0.94593572]]
Estimated regression coefficients: [[  1.70474504  15.04062212 -20.47216021]]
No. of iterations: 2612
Correctly predicted labels: 100



     
